# ============================================================================
# Voice AI Testing Framework - Environment Configuration Template
# ============================================================================
# Copy this file to .env and update with your actual values
# Never commit the .env file to version control!
# ============================================================================

# ============================================================================
# Application Configuration
# ============================================================================

# Environment: development, staging, production
ENVIRONMENT=development

# Enable debug mode (true/false)
ENABLE_DEBUG=true

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Application name
APP_NAME=Voice AI Testing Framework

# Application version
APP_VERSION=0.1.0

# ============================================================================
# API Server Configuration
# ============================================================================

# Backend API host
API_HOST=0.0.0.0

# Backend API port
API_PORT=8000

# Frontend URL (for CORS configuration)
FRONTEND_URL=http://localhost:3000

# Allowed CORS origins (comma-separated)
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:8000

# API request timeout (seconds)
API_TIMEOUT=30

# ============================================================================
# Database Configuration (PostgreSQL)
# ============================================================================

# PostgreSQL connection URL
# Format: postgresql://username:password@host:port/database
DATABASE_URL=postgresql://voice_ai_user:change-this-password@localhost:5432/voice_ai_testing

# Database host
DB_HOST=localhost

# Database port
DB_PORT=5432

# Database name
DB_NAME=voice_ai_testing

# Database user
DB_USER=voice_ai_user

# Database password
DB_PASSWORD=change-this-password

# Database connection pool size
DB_POOL_SIZE=20

# Database max overflow connections
DB_MAX_OVERFLOW=10

# Database pool timeout (seconds)
DB_POOL_TIMEOUT=30

# Enable SQL query logging (true/false)
DB_ECHO=false

# ============================================================================
# Redis Configuration
# ============================================================================

# Redis connection URL
# Format: redis://host:port/database
REDIS_URL=redis://localhost:6379/0

# Redis host
REDIS_HOST=localhost

# Redis port
REDIS_PORT=6379

# Redis database number (0-15)
REDIS_DB=0

# Redis password (if required)
REDIS_PASSWORD=your-redis-password-here

# Redis connection timeout (seconds)
REDIS_TIMEOUT=5

# Redis max connections in pool
REDIS_MAX_CONNECTIONS=50

# Cache TTL (time-to-live) in seconds
CACHE_TTL=3600

# ============================================================================
# RabbitMQ Configuration (Alternative Message Broker)
# ============================================================================

# RabbitMQ host
RABBITMQ_HOST=localhost

# RabbitMQ port
RABBITMQ_PORT=5672

# RabbitMQ default user
RABBITMQ_DEFAULT_USER=guest

# RabbitMQ default password
RABBITMQ_DEFAULT_PASS=guest

# RabbitMQ virtual host
RABBITMQ_VHOST=/

# ============================================================================
# Celery Configuration (Task Queue)
# ============================================================================

# Celery broker URL (uses Redis)
CELERY_BROKER_URL=redis://localhost:6379/1

# Celery result backend URL
CELERY_RESULT_BACKEND=redis://localhost:6379/2

# Task result expiration time (seconds)
CELERY_RESULT_EXPIRES=3600

# Task acknowledgement mode (late, early)
CELERY_TASK_ACKS_LATE=true

# Worker concurrency (number of worker processes)
CELERY_WORKER_CONCURRENCY=4

# ============================================================================
# SoundHound Voice AI Configuration
# ============================================================================

# SoundHound API key
SOUNDHOUND_API_KEY=your-soundhound-api-key-here

# SoundHound client ID
SOUNDHOUND_CLIENT_ID=your-soundhound-client-id-here

# SoundHound API endpoint
SOUNDHOUND_ENDPOINT=https://api.soundhound.com/v2

# SoundHound request timeout (seconds)
SOUNDHOUND_TIMEOUT=10

# SoundHound max retries
SOUNDHOUND_MAX_RETRIES=3

# SoundHound voice input format (PCM16, OPUS, etc.)
SOUNDHOUND_AUDIO_FORMAT=PCM16

# SoundHound sample rate (Hz)
SOUNDHOUND_SAMPLE_RATE=16000

# ----------------------------------------------------------------------------
# Houndify Mock Client Configuration
# ----------------------------------------------------------------------------

# Use mock Houndify client instead of real API (true/false)
# Default: true (uses mock for development)
USE_HOUNDIFY_MOCK=true

# Mock client type: "pattern" or "llm"
#   - pattern: Deterministic pattern-matching responses (default, no API calls)
#   - llm: Dynamic LLM-powered responses via OpenRouter (uses OPENROUTER_API_KEY)
HOUNDIFY_MOCK_TYPE=pattern

# LLM model for mock client (via OpenRouter, only used when HOUNDIFY_MOCK_TYPE=llm)
# Default: openai/gpt-4o-mini (other options: anthropic/claude-3-haiku, google/gemini-1.5-flash)
LLM_MOCK_MODEL=openai/gpt-4o-mini

# ============================================================================
# JWT Authentication Configuration
# ============================================================================

# JWT secret key (generate a strong random secret!)
# Use: python -c "import secrets; print(secrets.token_urlsafe(32))"
JWT_SECRET_KEY=CHANGE-ME-use-openssl-rand-base64-32-to-generate-a-strong-random-key

# JWT algorithm
JWT_ALGORITHM=HS256

# JWT token expiration time (minutes)
JWT_EXPIRATION_MINUTES=60

# JWT refresh token expiration (days)
JWT_REFRESH_EXPIRATION_DAYS=30

# Password hashing algorithm rounds
PASSWORD_HASH_ROUNDS=12

# ============================================================================
# AWS Configuration
# ============================================================================

# AWS access key ID
AWS_ACCESS_KEY_ID=your-aws-access-key-id

# AWS secret access key
AWS_SECRET_ACCESS_KEY=your-aws-secret-access-key

# AWS region
AWS_REGION=us-east-1

# AWS S3 bucket name for storing test artifacts
AWS_S3_BUCKET=voice-ai-testing-artifacts

# ==========================================================================
# Reporting Email Configuration
# ==========================================================================

# Comma-separated list of recipients for executive reports
REPORT_EMAIL_RECIPIENTS=qa@example.com,cto@example.com

# Sender email address used when mailing scheduled reports
REPORT_EMAIL_SENDER=reports@example.com

# SMTP server configuration for scheduled reports
REPORT_EMAIL_SMTP_HOST=smtp.example.com
REPORT_EMAIL_SMTP_PORT=587
REPORT_EMAIL_SMTP_USERNAME=
REPORT_EMAIL_SMTP_PASSWORD=
REPORT_EMAIL_USE_TLS=true
REPORT_EMAIL_TIMEOUT=30

# AWS S3 bucket for audio files
AWS_S3_AUDIO_BUCKET=voice-ai-testing-audio

# AWS S3 bucket region
AWS_S3_REGION=us-east-1

# S3 object expiration (days)
S3_OBJECT_EXPIRATION_DAYS=90

# Enable S3 versioning (true/false)
S3_VERSIONING_ENABLED=true

# ============================================================================
# MinIO Configuration (Local Development - TASK-114)
# ============================================================================
# MinIO is an S3-compatible object storage service for local development
# Set STORAGE_BACKEND=minio to use MinIO instead of AWS S3

# Storage backend to use: 's3' for AWS S3, 'minio' for local MinIO
STORAGE_BACKEND=minio

# MinIO endpoint URL (local development)
MINIO_ENDPOINT_URL=http://localhost:9000

# MinIO access key (matches docker-compose.yml)
MINIO_ACCESS_KEY=your-minio-access-key

# MinIO secret key (matches docker-compose.yml)
MINIO_SECRET_KEY=your-minio-secret-key

# MinIO region (can be any value for MinIO)
MINIO_REGION=us-east-1

# MinIO bucket names (auto-created by docker-compose)
MINIO_INPUT_AUDIO_BUCKET=input-audio
MINIO_OUTPUT_AUDIO_BUCKET=output-audio
MINIO_ARTIFACTS_BUCKET=artifacts

# MinIO console URL (for web UI access)
MINIO_CONSOLE_URL=http://localhost:9001

# ============================================================================
# Email/SMTP Configuration (for notifications)
# ============================================================================

# SMTP server host
SMTP_HOST=smtp.gmail.com

# SMTP server port
SMTP_PORT=587

# SMTP username
SMTP_USERNAME=your-email@example.com

# SMTP password
SMTP_PASSWORD=your-email-password

# SMTP use TLS (true/false)
SMTP_USE_TLS=true

# Email sender address
EMAIL_FROM=noreply@voice-ai-testing.com

# ============================================================================
# External Integrations
# ============================================================================

# GitHub OAuth Configuration (for GitHub Integration)
# To set up GitHub OAuth:
# 1. Go to GitHub Settings > Developer Settings > OAuth Apps > New OAuth App
# 2. Set Application name: Voice AI Testing Framework
# 3. Set Homepage URL: http://localhost:3001
# 4. Set Authorization callback URL: http://localhost:8000/api/v1/integrations/github/callback
# 5. Copy Client ID and Client Secret below
GITHUB_CLIENT_ID=
GITHUB_CLIENT_SECRET=
GITHUB_REDIRECT_URI=http://localhost:8000/api/v1/integrations/github/callback

# GitHub API token (for CI/CD integration - alternative to OAuth)
GITHUB_TOKEN=your-github-personal-access-token

# GitLab API token
GITLAB_TOKEN=your-gitlab-token

# Jira API credentials
JIRA_URL=https://your-company.atlassian.net
JIRA_USERNAME=your-jira-email@example.com
JIRA_API_TOKEN=your-jira-api-token

# Slack webhook URL (for notifications)
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL

# Slack channel for alerts
SLACK_ALERT_CHANNEL=#voice-ai-alerts

# ============================================================================
# Testing Configuration
# ============================================================================

# Test execution timeout (seconds)
TEST_EXECUTION_TIMEOUT=300

# Maximum concurrent test executions
MAX_CONCURRENT_TESTS=10

# Test result retention period (days)
TEST_RESULT_RETENTION_DAYS=90

# Enable test result caching (true/false)
ENABLE_TEST_CACHING=true

# Test data directory
TEST_DATA_DIR=./test_data

# ============================================================================
# Validation Configuration
# ============================================================================

# Validation accuracy threshold (0.0 - 1.0)
VALIDATION_ACCURACY_THRESHOLD=0.997

# ML model for semantic similarity
SEMANTIC_SIMILARITY_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Similarity score threshold (0.0 - 1.0)
SIMILARITY_THRESHOLD=0.85

# Enable human validation for low confidence results (true/false)
ENABLE_HUMAN_VALIDATION=true

# Human validation confidence threshold
HUMAN_VALIDATION_THRESHOLD=0.7

# ============================================================================
# LLM Ensemble Validation Configuration (OpenRouter)
# ============================================================================
# Three-stage pipeline: Dual Evaluators -> Curator -> Decision
# All models accessed via OpenRouter API for unified access
# See: https://openrouter.ai/models

# OpenRouter API key (required for LLM ensemble validation)
# Get yours at: https://openrouter.ai/keys
OPENROUTER_API_KEY=your-openrouter-api-key-here

# OpenRouter API base URL
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# ----------------------------------------------------------------------------
# Evaluator A Model (Google)
# ----------------------------------------------------------------------------
# Primary evaluator - fast and cost-effective
# Default: google/gemini-2.5-flash
#
# Alternatives:
#   google/gemini-2.0-flash       - Previous generation, still fast
#   google/gemini-1.5-flash       - Older but stable
#   google/gemini-1.5-pro         - More capable but slower
LLM_EVALUATOR_A_MODEL=google/gemini-2.5-flash

# ----------------------------------------------------------------------------
# Evaluator B Model (OpenAI)
# ----------------------------------------------------------------------------
# Secondary evaluator - independent second opinion
# Default: openai/gpt-4.1-mini
#
# Alternatives:
#   openai/gpt-4o-mini            - Previous fast model
#   openai/gpt-4o                 - More capable but slower/costlier
#   openai/gpt-4.1                - Full GPT-4.1 (expensive)
LLM_EVALUATOR_B_MODEL=openai/gpt-4.1-mini

# ----------------------------------------------------------------------------
# Curator Model (Anthropic)
# ----------------------------------------------------------------------------
# Tie-breaker - only called when evaluators disagree
# Default: anthropic/claude-sonnet-4.5 (best balance of speed/reasoning)
#
# Alternatives (faster, cheaper, less capable):
#   anthropic/claude-haiku-4.5   - Fastest Claude, 73% SWE-bench, $1/$5 per M
#   anthropic/claude-3.5-haiku   - Previous gen fast model
#   anthropic/claude-3-haiku     - Cheapest option
#
# Alternatives (slower, more capable):
#   anthropic/claude-opus-4.5    - Most intelligent, $5/$25 per M
#   anthropic/claude-opus-4      - Previous flagship
LLM_CURATOR_MODEL=anthropic/claude-sonnet-4.5

# ----------------------------------------------------------------------------
# LLM Ensemble Thresholds
# ----------------------------------------------------------------------------

# Consensus threshold - max score difference for high consensus (0.0-1.0)
# If evaluator scores differ by <= this amount, average them
LLM_CONSENSUS_THRESHOLD=0.15

# Extreme disagreement threshold - triggers human review (0.0-1.0)
# If evaluator scores differ by >= this amount, flag for human review
LLM_EXTREME_DISAGREEMENT_THRESHOLD=0.40

# Pass threshold - minimum score to pass validation (0.0-1.0)
LLM_PASS_THRESHOLD=0.80

# ----------------------------------------------------------------------------
# LLM Request Settings
# ----------------------------------------------------------------------------

# Temperature for LLM calls (0.0 = deterministic, 1.0 = creative)
LLM_TEMPERATURE=0.0

# Maximum tokens in LLM response
LLM_MAX_TOKENS=1024

# Request timeout in seconds
LLM_TIMEOUT=30

# Maximum retries on failure
LLM_MAX_RETRIES=3

# ----------------------------------------------------------------------------
# Validation Mode Configuration
# ----------------------------------------------------------------------------
# NOTE: Validation mode is now configured per-scenario, NOT globally!
# Each ScenarioScript has a validation_mode field that controls validation:
#
#   - houndify: Traditional CommandKind + ASR confidence validation only
#   - llm_ensemble: LLM-only behavioral validation (3-judge pipeline)
#   - hybrid: Both Houndify and LLM validation (recommended)
#
# Set validation_mode when creating/editing scenarios in the UI or via API.
# The default for new scenarios is 'hybrid'.

# ============================================================================
# Knowledge Base Generation Configuration
# ============================================================================

# LLM provider for KB article generation: openai, anthropic, or google
# If not set, uses template-based generation (no LLM calls)
KB_GENERATION_LLM_PROVIDER=openai

# LLM model for KB generation. If not set, uses provider default:
# - OpenAI: gpt-4o
# - Anthropic: claude-sonnet-4-5-20250929
# - Google: gemini-1.5-pro
KB_GENERATION_LLM_MODEL=gpt-4o

# ============================================================================
# Speech-to-Text Configuration (Local Whisper)
# ============================================================================
# Uses faster-whisper for local, offline speech-to-text transcription.
# 4x faster than OpenAI's original Whisper with same accuracy.
# No API costs, no latency, no rate limits.

# Whisper model size
# Options: tiny, base, small, medium, large-v3, turbo
# - tiny: 39M params, fastest, lower accuracy
# - base: 74M params, good balance (default)
# - small: 244M params, better accuracy
# - medium: 769M params, high accuracy
# - large-v3: 1.55B params, best accuracy
# - turbo: Optimized large-v3, best speed/accuracy tradeoff
STT_MODEL_SIZE=base

# Device for inference
# Options: cpu, cuda, auto
# - cpu: Use CPU (slower but works everywhere)
# - cuda: Use NVIDIA GPU (requires CUDA 12 + cuDNN 9)
# - auto: Auto-detect GPU, fallback to CPU (default)
STT_DEVICE=auto

# Directory to cache downloaded models
# Default: system cache directory
# STT_DOWNLOAD_ROOT=./models

# Pre-warm model at container startup (recommended for production)
# Downloads and loads model before accepting requests
# Set to false for faster container start (model loads on first request)
STT_PREWARM=true

# ============================================================================
# Monitoring & Observability
# ============================================================================

# Enable metrics collection (true/false)
ENABLE_METRICS=true

# Metrics export port
METRICS_PORT=9090

# Enable application performance monitoring (true/false)
ENABLE_APM=false

# APM service URL
APM_SERVICE_URL=http://localhost:8200

# APM service name
APM_SERVICE_NAME=voice-ai-testing

# ============================================================================
# Rate Limiting
# ============================================================================

# Default rate limit: requests per window for API
RATE_LIMIT_DEFAULT_REQUESTS=100

# Default rate limit window in seconds
RATE_LIMIT_DEFAULT_WINDOW=60

# Auth endpoint rate limit: requests per window (stricter for security)
RATE_LIMIT_AUTH_REQUESTS=10

# Auth endpoint rate limit window in seconds
RATE_LIMIT_AUTH_WINDOW=60

# Trusted proxy IPs/CIDRs for X-Forwarded-For validation
# Only these IPs can set X-Forwarded-For headers
RATE_LIMIT_TRUSTED_PROXIES=127.0.0.1,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16

# Rate limit: requests per minute for SoundHound API
SOUNDHOUND_RATE_LIMIT=50

# Enable rate limiting (true/false)
ENABLE_RATE_LIMITING=true

# ============================================================================
# Security Configuration
# ============================================================================

# Enable HTTPS only (true/false)
FORCE_HTTPS=false

# Enable CORS (true/false)
ENABLE_CORS=true

# Enable request validation (true/false)
ENABLE_REQUEST_VALIDATION=true

# Maximum request body size (bytes)
MAX_REQUEST_SIZE=10485760

# Enable SQL injection protection (true/false)
ENABLE_SQL_INJECTION_PROTECTION=true

# Session secret key (generate a strong random secret!)
SESSION_SECRET_KEY=your-session-secret-key-change-this

# Cookie secure flag (true/false - set to true in production)
COOKIE_SECURE=false

# ============================================================================
# Feature Flags
# ============================================================================

# Enable multi-language support (true/false)
ENABLE_MULTI_LANGUAGE=true

# Enable real-time dashboard updates (true/false)
ENABLE_REALTIME_DASHBOARD=true

# Enable test case versioning (true/false)
ENABLE_TEST_VERSIONING=true

# Enable automated regression testing (true/false)
ENABLE_AUTO_REGRESSION=true

# Enable defect detection (true/false)
ENABLE_DEFECT_DETECTION=true

# ============================================================================
# Deployment Configuration
# ============================================================================

# Deployment environment
DEPLOYMENT_ENV=local

# Enable auto-scaling (true/false)
ENABLE_AUTO_SCALING=false

# Minimum worker instances
MIN_WORKERS=1

# Maximum worker instances
MAX_WORKERS=10

# Target queued tasks per worker before scaling up
AUTO_SCALING_TARGET_TASKS_PER_WORKER=10

# Queue depth threshold to scale back to minimum workers
AUTO_SCALING_SCALE_DOWN_THRESHOLD=0

# Cooldown window between scaling actions (seconds)
AUTO_SCALING_COOLDOWN_SECONDS=30

# Queue monitored for auto-scaling decisions
AUTO_SCALING_QUEUE_NAME=default

# Maximum CPU percentage for execution tasks (set to 0 to disable)
EXECUTION_CPU_LIMIT_PERCENT=85

# Maximum memory usage in MB for execution tasks (set to 0 to disable)
EXECUTION_MEMORY_LIMIT_MB=2048

# Health check endpoint
HEALTH_CHECK_PATH=/health

# Readiness check endpoint
READINESS_CHECK_PATH=/ready

# ============================================================================
# Backup Configuration
# ============================================================================

# Enable automated backups (true/false)
ENABLE_AUTOMATED_BACKUPS=true

# Backup schedule (cron format)
BACKUP_SCHEDULE=0 2 * * *

# Backup retention period (days)
BACKUP_RETENTION_DAYS=30

# Backup storage location
BACKUP_STORAGE_URL=s3://voice-ai-testing-backups/

# ============================================================================
# Development & Debug Configuration
# ============================================================================

# Enable hot reload (development only)
ENABLE_HOT_RELOAD=true

# Enable SQL query profiling (true/false)
ENABLE_QUERY_PROFILING=false

# Enable request logging (true/false)
ENABLE_REQUEST_LOGGING=true

# Enable error stack traces (true/false)
ENABLE_STACK_TRACES=true

# Mock external services (development only)
MOCK_EXTERNAL_SERVICES=false

# ============================================================================
# End of Configuration
# ============================================================================
